*在多臂老虎机（multi-armed bandit，MAB）问题中，有一个拥有K根拉杆的老虎机，拉动每一根拉杆都对应一个关于奖励的概率分布R。我们每次拉动其中一根拉杆，就可以从该拉杆对应的奖励概率分布中获得一个奖励r。我们在各根拉杆的奖励概率分布未知的情况下，从头开始尝试，目标是在操作T次拉杆后获得尽可能高的累积奖励。*

#### 形式化描述
MAB 问题可以描述为一个元组 $<A,R>$ ,其中 $A$ 为动作集合，其中一个动作表示拉动任意一根拉杆，对应的 MAB 问题中就一共有 $K$ 个动作，可以记 $A = \{a_{1},……，a_K\}$ ; $R$ 是奖励概率分布，拉动每一根拉杆的动作都对应一个奖励概率分布 $R(r|a)$ 

假设每个时间步只能拉动一个拉杆，那么 MAB 问题的目标就是最大化一段时间步T 内累积的奖励: $max \Sigma_{t=1}^Tr_{t},r_{t} \sim R(\cdot|a_{t})$ 

##### 累计懊悔
在 $K$ 个动作中，一定存在一个动作，他的期望奖励不小于其他的动作的期望奖励，定义每一个动作的期望奖励和最大期望奖励的差距为**懊悔**。累计懊悔也就是一系列动作之后累计的懊悔总量，**因此**：*MAB 问题的目标为最大化累积奖励，等价于最小化累积懊悔。*

##### 期望奖励
计算每一个动作的期望奖励的过程：
- 对于，初始化计数器 和期望奖励估值
- **for**  **do**
	-  选取某根拉杆，该动作记为 $a_{t}$
	-  得到奖励 $r_{t}$
	-  更新计数器: $N(a_{t}) = N(a_{t}) + 1$
	-  更新期望奖励估值：$\hat{Q}{(a_{t})} = \hat{Q}{(a_{t})} + \frac{1}{N(a_{t})}[r_{t} - \hat{Q}{(a_{t})}]$
- **end for**

- 第四步的增量更新的推导：![[Pasted image 20250624161715.png]]
- 如果将所有数求和再除以次数，其缺点是每次更新的时间复杂度和空间复杂度均为$O(n)$。而采用增量式更新，时间复杂度和空间复杂度均为$O(1)$ 。

#### 算法框架

```python
class Solver:
    """ 多臂老虎机算法基本框架 """
    def __init__(self, bandit):
        self.bandit = bandit
        self.counts = np.zeros(self.bandit.K)  # 每根拉杆的尝试次数
        self.regret = 0.  # 当前步的累积懊悔
        self.actions = []  # 维护一个列表,记录每一步的动作
        self.regrets = []  # 维护一个列表,记录每一步的累积懊悔

    def update_regret(self, k):
        # 计算累积懊悔并保存,k为本次动作选择的拉杆的编号
        self.regret += self.bandit.best_prob - self.bandit.probs[k]
        self.regrets.append(self.regret)

    def run_one_step(self):
        # 返回当前动作选择哪一根拉杆,由每个具体的策略实现
        raise NotImplementedError

    def run(self, num_steps):
        # 运行一定次数,num_steps为总运行次数
        for _ in range(num_steps):
            k = self.run_one_step()
            self.counts[k] += 1
            self.actions.append(k)
            self.update_regret(k)
```

##### 探索和利用的平衡
***探索**（exploration）是指尝试拉动更多可能的拉杆，这根拉杆不一定会获得最大的奖励，但这种方案能够摸清楚所有拉杆的获奖情况。例如，对于一个 10 臂老虎机，我们要把所有的拉杆都拉动一下才知道哪根拉杆可能获得最大的奖励。**利用**（exploitation）是指拉动已知期望奖励最大的那根拉杆，由于已知的信息仅仅来自有限次的交互观测，所以当前的最优拉杆不一定是全局最优的。*

一个比较常用的思路是在开始时做比较多的探索，在对每根拉杆都有比较准确的估计后，再进行利用

#### 经典算法
##### $\epsilon - 贪心算法$
$\epsilon$-贪婪算法在完全贪婪算法的基础上添加了噪声，每次以概率$1 - \epsilon$选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率 $\epsilon$ 随机选择一根拉杆（探索），公式如下：![[Pasted image 20250624163511.png]]
*随着探索次数的不断增加，我们对各个动作的奖励估计得越来越准，此时我们就没必要继续花大力气进行探索。所以在 -贪婪算法的具体实现中，我们可以令  随时间衰减，即探索的概率将会不断降低。但是请注意， 不会在有限的步数内衰减至 0，因为基于有限步数观测的完全贪婪算法仍然是一个局部信息的贪婪算法，永远距离最优解有一个固定的差距。*

正常的 $\epsilon$-贪心算法的累计懊悔会随着时间步线性增长，在初期，累计懊悔增长较快，因为还处于探索阶段，经常选择非最优臂；到中后期，即使已经识别出来最优臂，依然会以$\epsilon$ 的概率犯错，假设非最优臂的平均懊悔为 $\Delta$ ,那么后期的累计懊悔期望大致为 $\epsilon \cdot \Delta \cdot t$ ，大致是一个线性函数
![[Pasted image 20250624165149.png]]

![[Pasted image 20250624164704.png]]
而随时间衰减的$\epsilon$ ,可以使得累计懊悔和时间步的关系变成次线性，优于固定值的 $\epsilon$-贪心算法，后期逐渐衰减的 $\epsilon$ 使得在识别出最优臂之后，犯错的概率逐渐减小，最终达成了次线性关系

![[Pasted image 20250624164842.png]]

##### 上置信界算法
*对于一台双臂老虎机，其中第一根拉杆只被拉动过一次，得到的奖励为 0；第二根拉杆被拉动过很多次，我们对它的奖励分布已经有了大致的把握。这时你会怎么做？或许你会进一步尝试拉动第一根拉杆，从而更加确定其奖励分布。这种思路主要是基于不确定性，因为此时第一根拉杆只被拉动过一次，它的不确定性很高。一根拉杆的不确定性越大，它就越具有探索的价值，因为探索之后我们可能发现它的期望奖励很大。我们在此引入不确定性度量 $U(a)$，它会随着一个动作被尝试次数的增加而减小。*

我们可以使用一种基于不确定性的策略来综合考虑现有的期望奖励估值和不确定性，其核心问题是如何估计不确定性。
**上置信界**（upper confidence bound，UCB）算法是一种经典的基于不确定性的策略算法，它的思想用到了一个非常著名的数学原理：**霍夫丁不等式**
（Hoeffding's inequality）。
令$X_{1},……，X_{n}$ 为 n 个独立同分布的随机变量，取值范围为$[0,1]$, 其经验期望为 $\bar{x}_{n} = \frac{1}{n}\Sigma_{j=1}^nX_{j}$ ,则有：
$$\mathbb{P}\{\mathbb{E}[X] \geq \bar{x}_n + u\} \leq e^{-2nu^2}$$
**我们可以考虑使用这个霍夫丁不等式来估计第 j 个动作的期望上界**。将$\hat{Q}_t(a)$代入$\bar{x}_t$，不等式中的参数$u = U_t(a)$代表不确定性度量。给定一个概率$p = e^{-2N_t(a)U_t(a)^2}$，根据上述不等式， $$Q_t(a) < \hat{Q}_t(a) + U_t(a)$$ 至少以概率$1-p$成立。当$p$很小时， $$Q_t(a) < \hat{Q}_t(a) + U_t(a)$$ 就以很大概率成立，$\hat{Q}_t(a) + U_t(a)$便是期望奖励上界。此时，上置信界算法便选取期望奖励上界最大的动作，即$a = \arg\max_{a \in \mathcal{A}} \left[\hat{Q}_t(a) + U_t(a)\right]$。 那其中$U_t(a)$具体是什么呢？根据等式$e^{-2N_t(a)U_t(a)^2}$，解之即得$U_t(a) = \sqrt{\frac{-\log p}{2N_t(a)}}$。 因此，设定一个概率$p$后，就可以计算相应的不确定性度量$U_t(a)$了。更直观地说，UCB 算法在每次选择拉杆前，先估计每根拉杆的期望奖励的上界，使得拉动每根拉杆的期望奖励只有一个较小的概率 p 超过这个上界，接着选出期望奖励上界最大的拉杆，从而选择最有可能获得最大期望奖励的拉杆。

##### 汤普森采样算法
*MAB 中还有一种经典算法——**汤普森采样**（Thompson sampling），先假设拉动每根拉杆的奖励服从一个特定的概率分布，然后根据拉动每根拉杆的期望奖励来进行选择。但是由于计算所有拉杆的期望奖励的代价比较高，汤普森采样算法使用采样的方式，即根据当前每个动作 a 的奖励概率分布进行一轮采样，得到一组各根拉杆的奖励样本，再选择样本中奖励最大的动作。**可以看出，汤普森采样是一种计算所有拉杆的最高奖励概率的蒙特卡洛采样方法。***

实际中，通常使用 Beta 分布对当前动作的奖励分布进行建模（因为在数学上最优：Beta分布是Bernoulli分布的共轭先验）

具体的代码实现：
```python
class ThompsonSampling(Solver):
    """ 汤普森采样算法,继承Solver类 """
    def __init__(self, bandit):
        super(ThompsonSampling, self).__init__(bandit)
        self._a = np.ones(self.bandit.K)  # 列表,表示每根拉杆奖励为1的次数
        self._b = np.ones(self.bandit.K)  # 列表,表示每根拉杆奖励为0的次数

    def run_one_step(self):
        samples = np.random.beta(self._a, self._b)  # 按照Beta分布采样一组奖励样本
        k = np.argmax(samples)  # 选出采样奖励最大的拉杆
        r = self.bandit.step(k)

        self._a[k] += r  # 更新Beta分布的第一个参数
        self._b[k] += (1 - r)  # 更新Beta分布的第二个参数
        return k


np.random.seed(1)
thompson_sampling_solver = ThompsonSampling(bandit_10_arm)
thompson_sampling_solver.run(5000)
print('汤普森采样算法的累积懊悔为：', thompson_sampling_solver.regret)
plot_results([thompson_sampling_solver], ["ThompsonSampling"])
```

#### 总结
- $\epsilon$-贪婪算法的累积懊悔是随时间线性增长的，而另外 3 种算法（ $\epsilon$-衰减贪婪算法、上置信界算法、汤普森采样算法）的累积懊悔都是随时间次线性增长的（具体为对数形式增长）。
- 多臂老虎机问题与强化学习的一大区别在于其与环境的交互并不会改变环境，即多臂老虎机的每次交互的结果和以往的动作无关，所以可看作**无状态的强化学习**（stateless reinforcement learning）。