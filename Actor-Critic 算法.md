 Actor-Critic 算法本质上还是基于策略的算法，只是会额外学习价值函数，从而帮助策略函数更好的学习。

# Actor-Critic
在 REINFORCE 算法中，目标函数的梯度中有一项轨迹回报，用于指导策略的更新。REINFOCE 算法用蒙特卡洛方法来估计 $Q(s,a)$ ，而在 AC 中采用拟合一个值函数来指导策略进行学习。

回顾一下，在REINFORCE 算法中，目标函数的梯度中有一项轨迹回报 $\psi(s,a)$，用于指导策略的更新。REINFOCE 算法用**蒙特卡洛方法**来估计 $\psi(s,a)$，能不能考虑拟合一个值函数来指导策略进行学习呢？这正是Actor-Critic 算法所做的。在策略梯度中，可以把梯度写成下面这个更加一般的形式： $$ \nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log\pi_\theta(a_t|s_t)\psi_t] $$ 其中，$\psi_t$ 可以有很多种形式： 
1. $\sum_{t'=t}^T \gamma^{t'-t}r_{t'}$：轨迹的总回报； 
2. $\sum_{t'=t}^T \gamma^{t'-t}r_{t'} - b(s_t)$：基准线版本的改进； 
3. $Q^{\pi_\theta}(s_t,a_t)$：动作价值函数； 
4. $A^{\pi_\theta}(s_t,a_t) = Q^{\pi_\theta}(s_t,a_t) - V^{\pi_\theta}(s_t)$：优势函数； 
5. $r_t + \gamma V^{\pi_\theta}(s_{t+1}) - V^{\pi_\theta}(s_t)$：时序差分残差。 


9.5 节提到REINFORCE 通过蒙特卡洛采样的方法对策略梯度的估计是无偏的，但是方差非常大。我们可以用形式(3)引入基线函数（baseline function）$b(s_t)$ 来减小方差。此外，我们也可以采用Actor-Critic 算法估计一个动作价值函数 $Q^{\pi_\theta}(s_t,a_t)$，代替蒙特卡洛采样得到的回报，这便是形式(4)。这个时候，我们可以把状态价值函数 $V^{\pi_\theta}(s_t)$ 作为基线，从 $Q^{\pi_\theta}(s_t,a_t)$ 函数减去这个 $V^{\pi_\theta}(s_t)$ 函数则得到了 $A^{\pi_\theta}(s_t,a_t)$ 函数，我们称之为优势函数（advantage function），这便是形式(5)。更进一步，我们可以利用 $Q^{\pi_\theta}(s_t,a_t) = r_t + \gamma \mathbb{E}[V^{\pi_\theta}(s_{t+1})]$ 等式得到形式(6)。 


本章将着重介绍形式(6)，即通过时序差分残差 $r_t + \gamma V^{\pi_\theta}(s_{t+1}) - V^{\pi_\theta}(s_t)$ 来指导策略梯度进行学习。事实上，用 $Q$ 值或者 $A$ 值本质上也是用奖励来进行指导，但是用神经网络进行估计的方法可以减小方差、提高鲁棒性。除此之外，REINFORCE 算法基于蒙特卡洛采样，只能在序列结束后进行更新，这同时也要求任务具有有限的步数，而Actor-Critic 算法则可以在每一步之后都进行更新，并且不对任务的步数做限制。 我们将Actor-Critic 分为两个部分：Actor（策略网络）和Critic（价值网络）：
* **Actor** 要做的是与环境交互，并在Critic 价值函数的指导下用策略梯度学习一个更好的策略。 
* **Critic** 要做的是通过Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助Actor 进行策略更新。 Actor 的更新采用策略梯度的原则，那Critic 如何更新呢？我们将Critic 价值网络表示为 $V_w(s)$，参数为 $w$。于是，我们可以采取时序差分残差的学习方式，对于单个数据 $(s_t, a_t, r_t, s_{t+1})$ 定义如下价值函数的损失函数： $$ L(w) = \frac{1}{2}(r_t + \gamma V_w(s_{t+1}) - V_w(s_t))^2 $$ 与DQN 中一样，我们采取类似于目标网络的方法，将上式中 $r_t + \gamma V_w(s_{t+1})$ 作为时序差分目标，不会产生梯度来更新价值函数。因此，价值函数的梯度为： $$ \nabla_w L(w) = -(r_t + \gamma V_w(s_{t+1}) - V_w(s_t)) \nabla_w V_w(s_t) $$ 然后使用梯度下降方法来更新Critic 价值网络参数即可。 

Actor-Critic 算法的具体流程如下： 
* 初始化策略网络参数 $\theta$，价值网络参数 $w$ 
* for 序列 $e=1 \rightarrow E$ do: 
	* 用当前策略 $\pi_\theta$ 采样轨迹 $\{s_1, a_1, r_1, \dots, s_T, a_T, r_T\}$ 
	* for $t=1 \rightarrow T$ do: 
		* 计算: $A_t = r_t + \gamma V_w(s_{t+1}) - V_w(s_t)$ 
		* 更新价值参数 $w \leftarrow w + \beta A_t \nabla_w V_w(s_t)$ 
		* 更新策略参数 $\theta \leftarrow \theta + \alpha \nabla_\theta \log \pi_\theta(a_t|s_t) A_t$ 
	* end for 


Actor-Critic 算法，它是基于值函数的方法和基于策略的方法的叠加。价值模块Critic 在策略模块Actor 采样的数据中学习分辨什么是好的动作，什么不是好的动作，进而指导Actor 进行策略更新。随着Actor 的训练的进行，其与环境交互所产生的数据分布也发生改变，这需要Critic 尽快适应新的数据分布并给出好的判别。 Actor-Critic 算法非常实用，后续章节中的TRPO、PPO、DDPG、SAC 等深度强化学习算法都是在Actor-Critic 框架下进行发展的。深入了解Actor-Critic 算法对读懂目前深度强化学习的研究热点大有裨益。

一些问题：

### 问题：为什么 TD 误差越小，就表示网络越好？

这个问题的核心在于：**TD误差是衡量我们价值网络预测的“自洽性”（Self-Consistency）的指标，而一个“好”的价值网络，其标志就是高度自洽。**

让我们把这个逻辑拆解开来：

#### 1. 终极目标：满足贝尔曼方程

一个**完美**的、**真实**的状态价值函数 $V^\pi(s)$ 应该满足**贝尔曼方程（Bellman Equation）**:

$$V^\pi(s) = \mathbb{E}_\pi [r_t + \gamma V^\pi(s_{t+1}) | s_t = s]$$

这个公式是什么意思？它描述了一种完美的“平衡”或“自洽”状态：
> 一个状态的价值，应该等于从它出发**走一步**所获得的期望奖励，加上经过折扣后，它所有可能的**下一个状态**的期望价值。

如果一个价值函数对**所有状态**都满足这个等式，那么它就是该策略下精确无误的价值函数。整个价值系统就达到了和谐统一。

#### 2. 现实情况：用 TD 误差来“抽样”贝尔曼误差

在实际训练中，我们的神经网络 $V_w(s)$ 还远不完美，它肯定不满足贝尔曼方程。而且，我们也没法对所有可能的下一步动作和状态求期望($\mathbb{E}_\pi$)。我们能做的就是通过**一次采样**来近似。

当我们从状态 $s_t$ 走出一步，到达 $s_{t+1}$ 并获得奖励 $r_t$ 时，我们就可以计算 **TD 误差**:

$$\delta_t = (r_t + \gamma V_w(s_{t+1})) - V_w(s_t)$$

对比一下就会发现，**TD 误差就是对贝尔曼误差的一次随机采样**！它衡量了在这一次具体的转移中，我们的网络预测在多大程度上**违背**了贝尔曼方程所描述的自洽性。

#### 总结

* **TD 误差小** $\implies$ 网络预测的值 $V_w(s_t)$ 与它的后继节点价值 $(r_t + \gamma V_w(s_{t+1}))$ 高度一致。
* **高度一致** $\implies$ 网络在大量的样本上近似满足了贝尔曼方程。
* **满足贝尔曼方程** $\implies$ 网络学会了一个自洽的、能够准确描述策略 $\pi$ 下回报期望的价值函数。
* **因此，TD 误差越小，网络就越好。**