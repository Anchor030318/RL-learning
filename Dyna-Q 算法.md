- rl 中，“模型”通常指与智能体交互的环境模型，即对环境的状态转移概率和奖励函数进行建模。根据是否具有环境模型，强化学习算法分为两种：
	- 基于模型的 rl：动态规划
	- 无模型的 rl：Sarsa 和 Q-learning

- 无模型的强化学习根据智能体与环境交互采样到的数据直接进行策略提升或者价值估计；
- 在基于模型的强化学习中，模型可以是事先知道的，也可以是根据智能体与环境交互采样到的数据学习得到的，然后用这个模型帮助策略提升或者价值估计

Dyna-Q 算法也是非常基础的基于模型的强化学习算法，不过它的环境模型是通过采样数据估计得到的。

强化学习算法有两个重要的评价指标：一个是**算法收敛后的策略在初始状态下的期望回报**，另一个是**样本复杂度，即算法达到收敛结果需要在真实环境中采样的样本数量**。

*基于模型的强化学习算法由于具有一个环境模型，智能体可以额外和环境模型进行交互，对真实环境中样本的需求量往往就会减少，因此通常会比无模型的强化学习算法具有更低的样本复杂度。但是，环境模型可能并不准确，不能完全代替真实环境，因此基于模型的强化学习算法收敛后其策略的期望回报可能不如无模型的强化学习算法。*

#### Dyna-Q
![[Pasted image 20250630151730.png|500]]
Dyna-Q 使用一种叫做 Q-planning 的方法来基于模型生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。Q-planning 每次选取一个曾经访问过的状态 $s$，采取一个曾经在该状态下执行过的动作 $a$，通过模型得到转移后的状态 $s'$以及奖励$r$，并根据这个模拟数据$(s,a,r,s')$，用 Q-learning 的更新方式来更新动作价值函数。

![[Pasted image 20250630151908.png|500]]

### Dyna-Q 的工作机制：学习与规划的结合

传统的无模型（Model-Free）强化学习算法，如 Q-Learning，完全依赖于与真实环境的直接交互来学习。每与环境交互一次（即获取一个样本 `(s, a, r, s')`），它就更新一次Q值。这种方式简单直接，但当真实交互的成本很高时（例如，机器人物理实验、昂贵的医学试验或耗时的游戏），学习效率会非常低下。

Dyna-Q 架构通过增加两个关键部分来解决这个问题：

1. **环境模型学习 (Model Learning)**：在与真实环境进行交互的同时，Dyna-Q 会利用这些真实的经验 `(s, a, r, s')` 来学习一个环境的模型 `Model(s, a)`。这个模型的作用是模拟真实环境：当你输入一个状态 `s` 和一个动作 `a` 时，模型会预测出下一个状态 `s'` 和奖励 `r`。
    
2. **规划 (Planning)**：这是 Dyna-Q 的精髓所在。在每一步真实交互之后，算法会额外进行 `n` 次“规划”步骤。在规划中，智能体并不是与真实环境交互，而是利用上面学到的**环境模型**来“凭空想象”或“模拟”出一些经验。具体来说，它会：
    
    - 随机选择一个**过去访问过**的状态 `s`。
        
    - 随机选择一个在状态 `s` 下**执行过**的动作 `a`。
        
    - 将 `(s, a)` 输入模型，得到模拟的奖励 `r` 和下一个状态 `s'`。
        
    - 使用这个**模拟的经验** `(s, a, r, s')` 来更新Q值，其更新方式与 Q-Learning 完全相同。
        

这个规划过程，因为是使用Q-Learning的更新规则来更新价值函数，所以也被形象地称为 **Q-planning**。