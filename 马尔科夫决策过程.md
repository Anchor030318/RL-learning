**马尔可夫决策过程**（Markov decision process，MDP）是强化学习的重要概念。

*如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程*

#### 马尔可夫过程
##### 随机过程
***随机过程**（stochastic process）是概率论的“动力学”部分。概率论的研究对象是静态的随机现象，而随机过程的研究对象是随时间演变的随机现象（例如天气随时间的变化、城市交通随时间的变化）。*
形式化表述：在随机过程中，随机现象在某时刻 $t$ 的取值是一个向量随机变量，用 $S_{t}$ 表示,所有可能的状态组成集合 $S$。随机现象是状态的变化过程，在 $t$ 时刻的状态通常取决于 $t$ 时刻之前的状态。将已知历史信息 $(S_{1},S_{2},……,S_{t})$ 时下一个时刻状态为 $S_{t+1}$ 的概率表示为 $P(S_{t+1}|S_{1},S_{2},……，S_{n})$ 

##### 马尔可夫性质
当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有**马尔可夫性质**（Markov property），用公式表示为 $P(S_{t+1})|S_{t} = P(S_{t+1}|S_{1},……,S_{t})$ 。

**当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。**

*马尔可夫性可以大大简化运算，因为只要当前状态可知，所有的历史信息都不再需要了，利用当前状态信息就可以决定未来。*

##### 马尔可夫过程
具有马尔可夫性质的随机过程，被称为马尔可夫过程，也被称为马尔可夫链。

形式化表述：可以用一个元组 $<S,P>$ 描述一个马尔可夫过程，其中 $S$ 是有限数量的状态集合，$P$ 是状态转移矩阵(state transition matrix)。假设一共有 $n$ 个状态，此时 $S = \{s_1, s_2, \dots, s_n\}$。状态转移矩阵 $P$ 定义了所有状态对之间的转移概率，即 

$P = \begin{bmatrix} P(s_1|s_1) & \cdots & P(s_n|s_1) \\ \vdots & \ddots & \vdots \\ P(s_1|s_n) & \cdots & P(s_n|s_n) \end{bmatrix}$

矩阵 $P$ 中第 $i$ 行第 $j$ 列元素 $P(s_j|s_i) = P(S_{t+1} = s_j | S_t = s_i)$ 表示从状态 $s_i$ 转移到状态 $s_j$ 的概率，我们称 $P(s'|s)$ 为状态转移函数。从某个状态出发，到达其他状态的概率和必须为 1，即状态转移矩阵 $P$ 的每一行的和为 1。

**生成状态序列**:给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态序列 (episode)，这个步骤也被叫做**采样 (sampling)**。例如，从 $s_1$ 出发，可以生成序列 $s_1 \to s_2 \to s_3 \to s_6$ 或序列 $s_1 \to s_1 \to s_2 \to s_3 \to s_4 \to s_5 \to s_3 \to s_6$ 等。生成这些序列的概率和状态转移矩阵有关。

##### 马尔可夫奖励过程
- 在马尔可夫过程的基础上加入奖励函数 和折扣因子，就可以得到**马尔可夫奖励过程**（Markov reward process）。一个马尔可夫奖励过程由 $<S,P,r,\gamma>$  构成，各个组成元素的含义如下所示：
	- $S$ 有限状态的集合
	- $P$ 状态转移矩阵
	- $r$ 是奖励函数，某个状态的奖励 $r(s)$ 指的是转移到这个状态时可以获得奖励的期望（模拟随机性）
	- $\gamma$ 是折扣因子，取值范围为 $[0,1)$ 。折扣因子 γ 是一个介于0和1之间的系数，用于在计算总回报时降低未来奖励的权重，它反映了未来奖励的不确定性和时间价值，并直接影响着智能体的决策是偏向于短期利益还是长期规划。接近 1 的 $\gamma$ 更关注长期的累计奖励，接近 0 的 $\gamma$ 更考虑短期奖励。 

###### 回报
在一个马尔可夫奖励过程中，从 $t$ 时刻状态 $S_t$ 开始，直到终止状态时，所有奖励的衰减之和称为回报 $G_t$ (Return)，公式如下： 
$G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k}$
###### 价值函数
*在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的**价值**（value）。所有状态的价值就组成了**价值函数**（value function），价值函数的输入为某个状态，输出为这个状态的价值。*

我们将价值函数写成 $V(s) = \mathbb{E}[G_t | S_t=s]$，展开为
$$\begin{align*} V(s) &= \mathbb{E}[G_t | S_t = s] \\ &= \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots | S_t = s] \\ &= \mathbb{E}[R_t + \gamma(R_{t+1} + \gamma R_{t+2} + \dots) | S_t = s] \\ &= \mathbb{E}[R_t + \gamma G_{t+1} | S_t = s] \\ &= \mathbb{E}[R_t + \gamma V(S_{t+1}) | S_t = s] \end{align*} $$在上述的最后一个等号中，一方面，即时奖励的期望正是奖励函数的输出，即 $\mathbb{E}[R_t | S_t=s] = r(s)$；另一方面，等式中剩余部分 $\mathbb{E}[\gamma V(S_{t+1}) | S_t = s]$ 可以根据从状态 $s$ 出发的转移概率得到，即可以得到 $$V(s) = r(s) + \gamma \sum_{s' \in S} P(s'|s) V(s')  $$上式就是马尔可夫奖励过程中非常有名的贝尔曼方程 (Bellman equation)，对每一个状态都成立。若一个马尔可夫奖励过程一共有 $n$ 个状态，即 $S = \{s_1, s_2, \dots, s_n\}$，我们将所有状态的价值表示成一个列向量 $\mathcal{V} = (V(s_1), V(s_2), \dots, V(s_n))^T$，同理，将奖励函数写成一个列向量 $\mathcal{R} = (r(s_1), r(s_2), \dots, r(s_n))^T$。于是我们可以将贝尔曼方程写成矩阵的形式：$$ \begin{bmatrix} V(s_1) \\ V(s_2) \\ \vdots \\ V(s_n) \end{bmatrix} = \begin{bmatrix} r(s_1) \\ r(s_2) \\ \vdots \\ r(s_n) \end{bmatrix} + \gamma \begin{bmatrix} P(s_1|s_1) & P(s_2|s_1) & \dots & P(s_n|s_1) \\ P(s_1|s_2) & P(s_2|s_2) & \dots & P(s_n|s_2) \\ \vdots & \vdots & \ddots & \vdots \\ P(s_1|s_n) & P(s_2|s_n) & \dots & P(s_n|s_n) \end{bmatrix} \begin{bmatrix} V(s_1) \\ V(s_2) \\ \vdots \\ V(s_n) \end{bmatrix}  $$我们可以直接根据矩阵运算求解，得到以下解析解： $$\begin{align*} \mathcal{V} &= \mathcal{R} + \gamma P \mathcal{V} \\ (I - \gamma P) \mathcal{V} &= \mathcal{R} \\ \mathcal{V} &= (I - \gamma P)^{-1} \mathcal{R} \end{align*} $$以上解析解的计算复杂度是 $O(n^3)$(主要是因为矩阵求逆的计算复杂度是$O(N^3)$)，其中 $n$ 是状态个数，因此这种方法只适用很小的马尔可夫奖励过程。求解较大规模的马尔可夫奖励过程中的价值函数时，可以使用动态规划 (dynamic programming) 算法、蒙特卡洛 (Monte Carlo) 方法等。
##### 马尔科夫决策过程
