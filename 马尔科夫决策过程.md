**马尔可夫决策过程**（Markov decision process，MDP）是强化学习的重要概念。

*如果要用强化学习去解决一个实际问题，第一步要做的事情就是把这个实际问题抽象为一个马尔可夫决策过程*

#### 马尔可夫过程
##### 随机过程
***随机过程**（stochastic process）是概率论的“动力学”部分。概率论的研究对象是静态的随机现象，而随机过程的研究对象是随时间演变的随机现象（例如天气随时间的变化、城市交通随时间的变化）。*
形式化表述：在随机过程中，随机现象在某时刻 $t$ 的取值是一个向量随机变量，用 $S_{t}$ 表示,所有可能的状态组成集合 $S$。随机现象是状态的变化过程，在 $t$ 时刻的状态通常取决于 $t$ 时刻之前的状态。将已知历史信息 $(S_{1},S_{2},……,S_{t})$ 时下一个时刻状态为 $S_{t+1}$ 的概率表示为 $P(S_{t+1}|S_{1},S_{2},……，S_{n})$ 

##### 马尔可夫性质
当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有**马尔可夫性质**（Markov property），用公式表示为 $P(S_{t+1})|S_{t} = P(S_{t+1}|S_{1},……,S_{t})$ 。

**当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。**

*马尔可夫性可以大大简化运算，因为只要当前状态可知，所有的历史信息都不再需要了，利用当前状态信息就可以决定未来。*

##### 马尔可夫过程
具有马尔可夫性质的随机过程，被称为马尔可夫过程，也被称为马尔可夫链。

形式化表述：可以用一个元组 $<S,P>$ 描述一个马尔可夫过程，其中 $S$ 是有限数量的状态集合，$P$ 是状态转移矩阵(state transition matrix)。假设一共有 $n$ 个状态，此时 $S = \{s_1, s_2, \dots, s_n\}$。状态转移矩阵 $P$ 定义了所有状态对之间的转移概率，即 

$P = \begin{bmatrix} P(s_1|s_1) & \cdots & P(s_n|s_1) \\ \vdots & \ddots & \vdots \\ P(s_1|s_n) & \cdots & P(s_n|s_n) \end{bmatrix}$

矩阵 $P$ 中第 $i$ 行第 $j$ 列元素 $P(s_j|s_i) = P(S_{t+1} = s_j | S_t = s_i)$ 表示从状态 $s_i$ 转移到状态 $s_j$ 的概率，我们称 $P(s'|s)$ 为状态转移函数。从某个状态出发，到达其他状态的概率和必须为 1，即状态转移矩阵 $P$ 的每一行的和为 1。

**生成状态序列**:给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态序列 (episode)，这个步骤也被叫做**采样 (sampling)**。例如，从 $s_1$ 出发，可以生成序列 $s_1 \to s_2 \to s_3 \to s_6$ 或序列 $s_1 \to s_1 \to s_2 \to s_3 \to s_4 \to s_5 \to s_3 \to s_6$ 等。生成这些序列的概率和状态转移矩阵有关。

##### 马尔可夫奖励过程(MRP)
- 在马尔可夫过程的基础上加入奖励函数 和折扣因子，就可以得到**马尔可夫奖励过程**（Markov reward process）。一个马尔可夫奖励过程由 $<S,P,r,\gamma>$  构成，各个组成元素的含义如下所示：
	- $S$ 有限状态的集合
	- $P$ 状态转移矩阵
	- $r$ 是奖励函数，某个状态的奖励 $r(s)$ 指的是转移到这个状态时可以获得奖励的期望（模拟随机性）
	- $\gamma$ 是折扣因子，取值范围为 $[0,1)$ 。折扣因子 γ 是一个介于0和1之间的系数，用于在计算总回报时降低未来奖励的权重，它反映了未来奖励的不确定性和时间价值，并直接影响着智能体的决策是偏向于短期利益还是长期规划。接近 1 的 $\gamma$ 更关注长期的累计奖励，接近 0 的 $\gamma$ 更考虑短期奖励。 

###### 回报
在一个马尔可夫奖励过程中，从 $t$ 时刻状态 $S_t$ 开始，直到终止状态时，所有奖励的衰减之和称为回报 $G_t$ (Return)，公式如下： 
$G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k}$
###### 价值函数
*在马尔可夫奖励过程中，一个状态的期望回报（即从这个状态出发的未来累积奖励的期望）被称为这个状态的**价值**（value）。所有状态的价值就组成了**价值函数**（value function），价值函数的输入为某个状态，输出为这个状态的价值。*

我们将价值函数写成 $V(s) = \mathbb{E}[G_t | S_t=s]$，展开为
$$\begin{align*} V(s) &= \mathbb{E}[G_t | S_t = s] \\ &= \mathbb{E}[R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \dots | S_t = s] \\ &= \mathbb{E}[R_t + \gamma(R_{t+1} + \gamma R_{t+2} + \dots) | S_t = s] \\ &= \mathbb{E}[R_t + \gamma G_{t+1} | S_t = s] \\ &= \mathbb{E}[R_t + \gamma V(S_{t+1}) | S_t = s] \end{align*} $$在上述的最后一个等号中，一方面，即时奖励的期望正是奖励函数的输出，即 $\mathbb{E}[R_t | S_t=s] = r(s)$；另一方面，等式中剩余部分 $\mathbb{E}[\gamma V(S_{t+1}) | S_t = s]$ 可以根据从状态 $s$ 出发的转移概率得到，即可以得到 $$V(s) = r(s) + \gamma \sum_{s' \in S} P(s'|s) V(s')  $$上式就是马尔可夫奖励过程中非常有名的贝尔曼方程 (Bellman equation)，对每一个状态都成立。若一个马尔可夫奖励过程一共有 $n$ 个状态，即 $S = \{s_1, s_2, \dots, s_n\}$，我们将所有状态的价值表示成一个列向量 $\mathcal{V} = (V(s_1), V(s_2), \dots, V(s_n))^T$，同理，将奖励函数写成一个列向量 $\mathcal{R} = (r(s_1), r(s_2), \dots, r(s_n))^T$。于是我们可以将贝尔曼方程写成矩阵的形式：$$ \begin{bmatrix} V(s_1) \\ V(s_2) \\ \vdots \\ V(s_n) \end{bmatrix} = \begin{bmatrix} r(s_1) \\ r(s_2) \\ \vdots \\ r(s_n) \end{bmatrix} + \gamma \begin{bmatrix} P(s_1|s_1) & P(s_2|s_1) & \dots & P(s_n|s_1) \\ P(s_1|s_2) & P(s_2|s_2) & \dots & P(s_n|s_2) \\ \vdots & \vdots & \ddots & \vdots \\ P(s_1|s_n) & P(s_2|s_n) & \dots & P(s_n|s_n) \end{bmatrix} \begin{bmatrix} V(s_1) \\ V(s_2) \\ \vdots \\ V(s_n) \end{bmatrix}  $$我们可以直接根据矩阵运算求解，得到以下解析解： $$\begin{align*} \mathcal{V} &= \mathcal{R} + \gamma P \mathcal{V} \\ (I - \gamma P) \mathcal{V} &= \mathcal{R} \\ \mathcal{V} &= (I - \gamma P)^{-1} \mathcal{R} \end{align*} $$以上解析解的计算复杂度是 $O(n^3)$(主要是因为矩阵求逆的计算复杂度是$O(N^3)$)，其中 $n$ 是状态个数，因此这种方法只适用很小的马尔可夫奖励过程。求解较大规模的马尔可夫奖励过程中的价值函数时，可以使用动态规划 (dynamic programming) 算法、蒙特卡洛 (Monte Carlo) 方法等。
##### 马尔科夫决策过程(MDP)
*马尔可夫过程和马尔可夫奖励过程都是自发改变的随机过程；而如果有一个外界的“刺激”来共同改变这个随机过程，就有了**马尔可夫决策过程**（Markov decision process，MDP）。我们将这个来自外界的刺激称为**智能体**（agent）的动作，在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程（MDP）*

形式化表述：
- MDP 由元组 $<S,A,P,r,\gamma>$ 构成，其中：
	- $S$ 有限状态的集合
	- $A$ 是动作集合
	- $P(s'|s,a)$ 是状态转移函数，表示在状态 $s$ 执行动作 $a$ 之后到达状态 $s'$ 的概率
	- $r(s,a)$ 是奖励函数，此时奖励可以同时取决于状态 $s$ 和动作 $a$ ,如果奖励函数只取决于状态时，则退化为 $r(s)$ ；具体的意思是智能体在状态 $s$ ,如果采取动作 $a$ ，可以获得奖励 $r(s,a)$ 
	- $\gamma$ 是折扣因子

智能体和环境 MDP 的交互：
![[Pasted image 20250625215821.png|450]]
##### 策略
- 智能体的**策略**（Policy）通常用字母 $\pi$ 表示。策略 $\pi(a|s) = P(A_{t} = a|S_{t} = s)$是一个函数，表示在输入状态 $s$ 情况下采取动作 $a$ 的概率。
	- **确定性策略**：在每个状态时只输出一个确定性的动作，即只有该动作的概率为 1，其他动作的概率为 0
	- **随机性策略**：在每个状态时输出的是关于动作的概率分布，然后根据该分布进行采样就可以得到一个动作

*在 MDP 中，由于马尔可夫性质的存在，策略只需要与当前状态有关，不需要考虑历史状态。回顾一下在 MRP 中的价值函数，在 MDP 中也同样可以定义类似的价值函数。但此时的价值函数与策略有关，这意为着对于两个不同的策略来说，它们在同一个状态下的价值也很可能是不同的。这很好理解，因为不同的策略会采取不同的动作，从而之后会遇到不同的状态，以及获得不同的奖励，所以它们的累积奖励的期望也就不同，即状态价值不同。*

###### 状态价值函数
我们用 $V^{\pi}(s)$ 表示在 MDP 中基于策略 $\pi$ 的状态价值函数 (state-value function)，定义为从状态 $s$ 出发遵循策略 $\pi$ 能获得的期望回报，数学表达为： $$V^{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s]$$
###### 动作价值函数
不同于 MRP，在 MDP 中，由于动作的存在，我们额外定义一个动作价值函数 (action-value function)。我们用 $Q^{\pi}(s, a)$ 表示在 MDP 遵循策略 $\pi$ 时，对当前状态 $s$ 执行动作 $a$ 得到的期望回报： $$Q^{\pi}(s, a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t = a]$$状态价值函数和动作价值函数之间的关系：在使用策略 $\pi$ 中，状态 $s$ 的价值等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的价值相乘再求和的结果：$$V^{\pi}(s) = \sum_{a \in A} \pi(a|s) Q^{\pi}(s, a)$$使用策略 $\pi$ 时，状态 $s$ 下采取动作 $a$ 的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积：$$Q^{\pi}(s, a) = r(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) V^{\pi}(s')$$
###### 贝尔曼期望方程
在贝尔曼方程中加上“期望”二字是为了与接下来的贝尔曼最优方程进行区分。我们通过简单推导就可以分别得到两个价值函数的贝尔曼期望方程 (Bellman Expectation Equation)：
$$\begin{align*} V^{\pi}(s) &= \mathbb{E}_{\pi}[R_t + \gamma V^{\pi}(S_{t+1}) | S_t = s] \\ &= \sum_{a \in A} \pi(a|s) \left( r(s, a) + \gamma \sum_{s' \in S} p(s'|s, a) V^{\pi}(s') \right) \end{align*}$$$$\begin{align*} Q^{\pi}(s, a) &= \mathbb{E}_{\pi}[R_t + \gamma Q^{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a] \\ &= r(s, a) + \gamma \sum_{s' \in S} p(s'|s, a) \sum_{a' \in A} \pi(a'|s') Q^{\pi}(s', a') \end{align*}$$
实际进行计算的时候，可以先将 MDP 转化为 MRP，计算状态价值函数（求解析解的方式），得到了状态价值函数之后再计算动作价值函数

##### 蒙特卡洛方法
**蒙特卡洛方法**（Monte-Carlo methods）也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用蒙特卡洛方法时，我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。

我们可以考虑利用蒙特卡洛方法来估计一个策略在一个 MDP 上的状态价值函数：一个状态的价值是它的期望回报，那么一个很直观的想法就是用策略在 MDP 上采样很多条序列，计算从这个状态出发的回报再求其期望：$$V^{\pi}(s) = \mathbb{E}_{\pi}[G_t | S_t = s] \frac{1}{N}\sum_{i = 1}^{N}G_{t}^{(i)}$$

*在一条序列中，可能没有出现过这个状态，可能只出现过一次这个状态，也可能出现过很多次这个状态。*
- 有两种估计方法：
	- 对于采样序列中的状态，每次出现都计算回报
	- 对于采样序列中的状态，只在第一次出现时计算回报，后续该状态再次出现时，忽略

(1) 使用策略$\pi$采样若干条序列： 
$$\begin{equation} s_0^{(i)} \xrightarrow{a_0^{(i)}} r_0^{(i)}, s_1^{(i)} \xrightarrow{a_1^{(i)}} r_1^{(i)}, s_2^{(i)} \xrightarrow{a_2^{(i)}} \cdots \xrightarrow{a_{T-1}^{(i)}} r_{T-1}^{(i)}, s_T^{(i)} \end{equation} $$(2) 对每一条序列中的每一个时间步$t$的状态$s$进行如下操作： 
- 更新状态$s$的计数器 $N(s) \leftarrow N(s) + 1$
- 更新状态$s$的总回报 $M(s) \leftarrow M(s) + G_t$
(3) 每一个状态的价值被估计为回报的平均值 $V(s) = M(s)/N(s)$。 根据大数定律，当$N(s) \to \infty$，有$V(s) \to V^{\pi}(s)$。计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态$s$和对应回报$G$，进行如下计算： 
- $N(s) \leftarrow N(s) + 1$ 
- $V(s) \leftarrow V(s) + \frac{1}{N(s)}(G - V(s))$

#### 占用度量
*不同策略会使智能体访问到不同概率分布的状态，这会影响到策略的价值函数。*

首先我们定义MDP的初始状态分布为$\nu_0(s)$，在有些资料中，初始状态分布会被定义进MDP的组成元素中。我们用$P_t^{\pi}(s)$表示采取策略$\pi$使得智能体在$t$时刻状态为$s$的概率，所以我们有$P_0^{\pi}(s) = \nu_0(s)$，然后就可以定义一个策略的状态访问分布(state visitation distribution)：$$\begin{equation} \nu^{\pi}(s) = (1-\gamma) \sum_{t=0}^{\infty} \gamma^t P_t^{\pi}(s) \end{equation}$$  
其中，$1-\gamma$是用来使得概率加和为1的归一化因子。(可以两边进行求和，进行一下和式变换就可以验证)

状态访问概率表示一个策略和MDP交互会访问到的状态的分布。需要注意的是，理论上在计算该分布时需要交互到无穷步之后，但实际上智能体和MDP的交互在一个序列中是有限的。不过我们仍然可以用以上公式来表达状态访问概率的思想，状态访问概率有如下性质： $$\begin{equation} \nu^{\pi}(s') = (1-\gamma)\nu_0(s') + \gamma \int P(s'|s,a)\pi(a|s)\nu^{\pi}(s)dsda \end{equation} $$此外，我们还可以定义策略的**占用度量(occupancy measure)**： $$\begin{equation} \rho^{\pi}(s,a) = (1-\gamma) \sum_{t=0}^{\infty} \gamma^t P_t^{\pi}(s)\pi(a|s) \end{equation} $$它表示动作状态对$(s,a)$被访问到的概率。二者之间存在如下关系： $$\begin{equation} \rho^{\pi}(s,a) = \nu^{\pi}(s)\pi(a|s) \end{equation} $$进一步得出如下两个定理:
- 定理1：智能体分别以策略$\pi_1$和$\pi_2$和同一个MDP交互得到的占用度量$\rho^{\pi_1}$和$\rho^{\pi_2}$满足 $$\begin{equation} \rho^{\pi_1} = \rho^{\pi_2} \Leftrightarrow \pi_1 = \pi_2 \end{equation}$$
- 定理2：给定一合法占用度量$\rho$，可生成该占用度量的唯一策略是 $$\begin{equation} \pi_{\rho} = \frac{\rho(s,a)}{\sum_{a'} \rho(s,a')} \end{equation}$$

换句话说，策略和占用度量等价，可以用占用度量来表征策略；实际中，可以用状态动作对出现的频率来估计实际概率。示例代码：
```python
def occupancy(episodes, s, a, timestep_max, gamma):
    ''' 计算状态动作对（s,a）出现的频率,以此来估算策略的占用度量 '''
    rho = 0
    total_times = np.zeros(timestep_max)  # 记录每个时间步t各被经历过几次
    occur_times = np.zeros(timestep_max)  # 记录(s_t,a_t)=(s,a)的次数
    for episode in episodes:
        for i in range(len(episode)):
            (s_opt, a_opt, r, s_next) = episode[i]
            total_times[i] += 1
            if s == s_opt and a == a_opt:
                occur_times[i] += 1
    for i in reversed(range(timestep_max)):
        if total_times[i]:
            rho += gamma**i * occur_times[i] / total_times[i]
    return (1 - gamma) * rho


gamma = 0.5
timestep_max = 1000

episodes_1 = sample(MDP, Pi_1, timestep_max, 1000)
episodes_2 = sample(MDP, Pi_2, timestep_max, 1000)
rho_1 = occupancy(episodes_1, "s4", "概率前往", timestep_max, gamma)
rho_2 = occupancy(episodes_2, "s4", "概率前往", timestep_max, gamma)
print(rho_1, rho_2)
```

#### 最优策略
*强化学习的目标通常是找到一个策略，使得智能体从初始状态出发能获得最多的期望回报。*

**定义策略之间的偏序关系：当且仅当对于任意的状态 $s$ 都有 $V^\pi(s) \geq V^{\pi'}(s)$ ，记 $\pi \geq \pi'$。**

于是，在一个有限状态和动作集合的 MDP 中，至少存在一个**最优策略**，把所有的最优策略都表示为$\pi^*(s)$ 。

最优策略都有相同的状态价值函数，我们称之为最优状态价值函数，表示为： $$ V^*(s) = \max_{\pi} V^{\pi}(s), \quad \forall s \in S $$ 同理，我们定义最优动作价值函数： $$ Q^*(s,a) = \max_{\pi} Q^{\pi}(s,a), \quad \forall s \in S, a \in A $$ 为了使$Q^*(s,a)$最大，我们需要在当前的状态动作对$(s,a)$之后都执行最优策略。于是我们得到了最优状态价值函数和最优动作价值函数之间的关系： $$ Q^*(s,a) = r(s,a) + \gamma \sum_{s' \in S} P(s'|s,a)V^*(s') $$ 这与在普通策略下的状态价值函数和动作价值函数之间的关系是一样的。另一方面，最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值： $$ V^*(s) = \max_{a \in A} Q^*(s,a) $$
##### 贝尔曼最优方程
根据$V^*(s)$和$Q^*(s,a)$的关系，我们可以得到贝尔曼最优方程 (Bellman optimality equation)： $$ V^*(s) = \max_{a \in A} \{r(s,a) + \gamma \sum_{s' \in S} p(s'|s,a)V^*(s')\} $$ $$ Q^*(s,a) = r(s,a) + \gamma \sum_{s' \in S} p(s'|s,a) \max_{a' \in A} Q^*(s',a') $$