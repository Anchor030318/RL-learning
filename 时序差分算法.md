*对于大部分强化学习现实场景（例如电子游戏或者一些复杂物理环境），其马尔可夫决策过程的状态转移概率是无法写出来的，也就无法直接进行动态规划。在这种情况下，智能体只能和环境进行交互，通过采样到的数据来学习，这类学习方法统称为**无模型的强化学习**（model-free reinforcement learning）。*

无模型的强化学习中的两大经典算法：Sarsa 和 Q-learning，它们都是基于**时序差分**（temporal difference，TD）的强化学习算法。

**在线策略学习和离线策略学习**：通常来说，*在线策略学习要求使用在当前策略下采样得到的样本进行学习，一旦策略被更新，当前的样本就被放弃了*，就好像在水龙头下用自来水洗手；*而离线策略学习使用经验回放池将之前采样得到的样本收集起来再次利用*，就好像使用脸盆接水后洗手。因此，**离线策略学习往往能够更好地利用历史数据，并具有更小的样本复杂度（算法达到收敛结果需要在环境中采样的样本数量），这使其被更广泛地应用**。

#### 时序差分方法
- 蒙特卡洛（MC）和时序差分（TD）的区别：
	- **蒙特卡洛 (MC):** 是“**事后诸葛亮**”。必须等待一个完整的事件（episode）结束，拿到最终的“成败结果”（即完整的真实回报 Gt​），然后用这个最终结果去反思和更新路径上每一步的价值。
	    
	    - **优点:** 评估是**无偏的**，因为使用的是真实、完整的回报。
	        
	    - **缺点:** **方差很高**（运气成分大），且学习**效率低**（必须等到底）。
	        
	- **时序差分 (TD):** 是“**边走边学，及时调整**”。每走一步，就根据已经拿到的即时奖励 `r` 和对下一步价值的**当前估计** `$Q(s',a')$`，来更新上一步的价值。这个用“估计值”来更新“估计值”的方法，称为**自举 (Bootstrapping)**。
	    
	    - **优点:** **方差低**，学习**效率高**（可以单步更新，在线学习）。
	        
	    - **缺点:** 评估是**有偏的**，因为依赖于一个尚不准确的估计值。
#### Sarsa算法
利用 TD 来估计动作价值函数：用当前动作的奖励和下一步动作的估计值来估计当前动作
$$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)] $$ 然后我们用贪婪算法来选取在某个状态下动作价值最大的那个动作，即 $\arg\max_a Q(s, a)$。这样似乎已经形成了一个完整的强化学习算法：用贪婪算法根据动作价值选取动作来和环境交互，再根据得到的数据用时序差分算法更新动作价值估计。 然而这个简单的算法存在两个需要进一步考虑的问题。第一，如果要用时序差分算法来准确地估计策略的状态价值函数，我们需要用极大量的样本来进行更新。但实际上我们可以忽略这一点，直接用一些样本来评估策略，然后就可以更新策略了。我们可以这么做的原因是策略提升可以在策略评估未完全进行的情况下进行，回顾一下，价值迭代（参见 4.4 节）就是这样，这其实是广义策略迭代 (generalized policy iteration) 的思想。第二，如果在策略提升中一直根据贪婪算法得到一个确定性策略，可能会导致某些状态动作对$(s, a)$永远没有在序列中出现，以至于无法对其动作价值进行估计，进而无法保证策略提升后的策略比之前的好。我们在第 2 章中对此有详细讨论。简单常用的解决方案是不再一味使用贪婪算法，而是采用一个$\epsilon$-贪婪策略：有$1-\epsilon$的概率采用动作价值最大的那个动作，另外有$\epsilon$的概率从动作空间中随机采取一个动作，其公式表示为： $$ \pi(a|s) = \begin{cases} \epsilon/|A| + 1 - \epsilon & \text{如果 } a = \arg\max_{a'} Q(s, a') \\ \epsilon/|A| & \text{其他动作} \end{cases} $$ 现在，我们就可以得到一个实际的基于时序差分方法的强化学习算法。这个算法被称为 Sarsa，因为它的动作价值更新用到了当前状态$s$、当前动作$a$、获得的奖励$r$、下一个状态$s'$和下一个动作$a'$，将这些符号拼接后就得到了算法名称。Sarsa 的具体算法如下：
- 初始化$Q(s, a)$
- **for** 序列 $e = 1 \rightarrow E$ **do**: 
	- 得到初始状态$s$ 
	- 用$\epsilon$-greedy 策略根据$Q$选择当前状态$s$下的动作$a$ 
	- **for** 时间步 $t = 1 \rightarrow T$ **do**:  
		- 得到环境反馈的$r, s'$ 
		- 用$\epsilon$-greedy 策略根据$Q$选择当前状态$s'$下的动作$a'$ 
		- $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma Q(s', a') - Q(s, a)]$ - $s \leftarrow s', a \leftarrow a'$ 
	- **end for** 
- **end for**
#### 多步Sarsa算法
相比于 Sarsa 算法，多采样几步

蒙特卡洛方法利用当前状态之后每一步的奖励而不使用任何价值估计，时序差分算法只利用一步奖励和下一个状态的价值估计。那它们之间的区别是什么呢？总的来说，蒙特卡洛方法是无偏 (unbiased) 的，但是具有比较大的方差，因为每一步的状态转移都有不确定性，而每一步状态采取的动作所得到的不一样的奖励最终都会加起来，这会极大影响最终的价值估计；时序差分算法具有非常小的方差，因为只关注了一步状态转移，用到了一步的奖励，但是它是有偏的，因为用到了下一个状态的价值估计而不是其真实的价值。那有没有什么方法可以结合二者的优势呢？答案是**多步时序差分**！多步时序差分的意思是使用$n$步的奖励，然后使用之后状态的价值估计。用公式表示，将 $$ G_t = r_t + \gamma Q(s_{t+1}, a_{t+1}) $$ 替换成 $$ G_t = r_t + \gamma r_{t+1} + \dots + \gamma^n Q(s_{t+n}, a_{t+n}) $$ 于是，相应存在一种多步 Sarsa 算法，它把 Sarsa 算法中的动作价值函数的更新公式（参见 5.3 节） $$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)] $$ 替换成 $$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_t + \gamma r_{t+1} + \dots + \gamma^n Q(s_{t+n}, a_{t+n}) - Q(s_t, a_t)] $$

#### Q-learning 算法
除了 Sarsa，还有一种非常著名的基于时序差分算法的强化学习算法——Q-learning。Q-learning 和 Sarsa 的最大区别在于 Q-learning 的时序差分更新方式为 $$ Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [R_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t)] $$ Q-learning 算法的具体流程如下： 
- 初始化$Q(s, a)$ 
	- **for** 序列 $e = 1 \rightarrow E$ **do**: 
		- 得到初始状态$s$ 
		- **for** 时间步 $t = 1 \rightarrow T$ **do**: 
			- 用$\epsilon$-greedy 策略根据$Q$选择当前状态$s$下的动作$a$ 
			- 得到环境反馈的$r, s'$ 
			- $Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]$ 
			- $s \leftarrow s'$ 
		- **end for** 
	- **end for** 
- 我们可以用价值迭代的思想来理解 Q-learning，**即 Q-learning 是直接在估计$Q^*$**，因为动作价值函数的贝尔曼最优方程是 $$ Q^*(s, a) = r(s, a) + \gamma \sum_{s' \in S} P(s'|s, a) \max_{a'} Q^*(s', a') $$ 而 Sarsa 估计当前$\epsilon$-贪婪策略的动作价值函数。需要强调的是，Q-learning 的更新并非必须使用当前贪心策略$\arg\max_a Q(s,a)$采样得到的数据，因为给定任意$(s, a, r, s')$都可以直接根据更新公式来更新Q，为了探索，我们通常使用一个$\epsilon$-贪婪策略来与环境交互。Sarsa 必须使用当前$\epsilon$-贪婪策略采样得到的数据，因为它的更新中用到的$Q(s', a')$的$a'$是当前策略在$s'$下的动作。我们称 Sarsa 是在策略略 (on-policy) 算法，称 Q-learning 是离策略略 (off-policy) 算法，这两个概念强化学习中非常重要。

#### 在线策略算法与离线策略算法 
我们称采样数据的策略为**行为策略 (behavior policy)**，称用这些数据来**更新的策略为目标策略** (target policy)。在线策略 (on-policy) 算法表示**行为策略和目标策略是同一个策略**；而离线策略 (off-policy) 算法表示**行为策略和目标策略不是同一个策略。**

Sarsa 是典型的在线策略算法，而 Q-learning 是典型的离线策略算法。判断二者差别的一个重要手段是看计算时序差分价值目标的数据是否来自当前策略。具体而言： 
- 对于 Sarsa，它的更新公式必须使用来自当前策略采样得到的五元组$(s, a, r, s', a')$，因此它是在线策略学习方法。 
- 对于 Q-learning，它的更新公式使用的是四元组$(s, a, r, s')$来更新当前状态动作对的价值$Q(s,a)$，数据中的$s$和$a$是给定的条件，$r$和$s'$则由环境采样得到，该四元组并不需要一定是当前策略采样得到的数据，也可以来自行为策略，因此它是离线策略算法。