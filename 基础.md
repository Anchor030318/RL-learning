强化学习中 Agent 和环境之间会进行迭代式交互
#### 智能体
智能体有三种关键要素：感知、决策和奖励
- 智能体感知环境的状态
- 智能体根据当前的状态计算出达到目标需要采取的动作的过程称为决策；策略是智能体最终体现的智能形式，是不同智能体之间的核心区别
- 环境根据状态和智能体采取的动作，产生一个**标量信号**作为奖励反馈，这个标量信号衡量智能体在这一轮动作的好坏；整个强化学习迭代的过程的最终目标是最大化累计奖励期望

面向决策任务的强化学习和面向预测任务的有监督学习：
- 决策任务涉及**多轮交互，即序贯决策**；而预测任务总是**单轮的独立任务**（如果决策是单轮的，可以转化为“判别最优动作”的预测任务）
- 因为决策任务是多轮的，Agent 在每轮做决策的时候就需要考虑决策对于未来环境的变化可能造成的影响，即当前最大奖励反馈的动作，在长期来看并不一定是最优的

#### 环境
强化学习中的环境是动态的，通常可以使用随机过程来刻画动态环境；而对于随机过程来说，最关键的要素就是**状态以及状态转移的条件概率分布**（eg：*这就好比一个微粒在水中的布朗运动可以由它的起始位置以及下一刻的位置相对当前位置的条件概率分布来刻画*）

如果在原始环境的自然演变中加入一个外来的干扰因素，即智能体的动作，那么环境的下一刻状态的概率分布将由当前状态和智能体的动作来共同决定$$下一状态 ～ P(|当前状态，智能体动作)$$
面向决策任务的智能体与环境的每一轮交互都伴随着两方面的随机性：一是智能体决策的随机性；二是环境基于当前状态和智能体决策动作来采样下一刻状态的随机性

#### 目标
最大化每一轮迭代过程中的累积奖励。

价值的计算较为复杂，需要对交互过程中每一轮智能体采取动作的概率分布和环境状态转移的概率分布做积分运算。

**强化学习和有监督学习的学习目标其实是一致的，即在某个数据分布下优化一个分数值的期望**，不过强化学习和有监督学习的优化途径是不同的

#### 数据
- 在有监督学习中，**训练数据的数据分布是完全不变的**
- 在强化学习中，数据是在智能体和环境交互的过程中得到的，当前智能体的训练数据来自于之前智能体的决策结果，因此，**智能体的策略不同，与环境交互所产生的数据分布就不同**

强化学习中，有一个关于数据分布的概念——**占用度量**，归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的状态动作对（state-action pair）的概率分布。

占用度量有一个很重要的性质：*给定两个策略及其与一个动态环境交互得到的两个占用度量，那么当且仅当这两个占用度量相同时，这两个策略相同。也就是说，如果一个智能体的策略有所改变，那么它和环境交互得到的占用度量也会相应改变。* 

- **由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。** 

#### 总结 RL 和监督学习的区别
- 有监督学习和强化学习的优化目标相似，即都是在优化某个数据分布下的一个分数值的期望。
- ![[Pasted image 20250624154225.png]]
- ![[Pasted image 20250624154233.png]]
- 二者优化的途径是不同的，有监督学习直接通过优化模型对于数据特征的输出来优化目标，即修改目标函数而数据分布不变；强化学习则通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，即修改数据分布而目标函数不变。

- **一般的有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小；**
- **强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望。**