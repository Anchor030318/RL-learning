Q-learning 算法中，我们以矩阵的方式建立了一张存储每个状态下所有动作 $Q$ 值的表格。表格中的每一个动作价值 $Q(s,a)$ 表示在状态 $s$ 下选择动作 $a$ 然后继续遵循某一策略预期能够得到的期望回报。

然而，这种用表格存储动作价值的做法只在环境的状态和动作都是离散的，并且空间都比较小的情况下适用，我们之前进行代码实战的几个环境都是如此（如悬崖漫步）。当状态或者动作数量非常大的时候，这种做法就不适用了。

对于这种情况，我们需要用函数拟合的方法来估计 $Q$ 值，即将这个复杂的 $Q$ 值表格视作数据，使用一个参数化的函数 $Q_{\theta}$来拟合这些数据。很显然，这种函数拟合的方法存在一定的精度损失，因此被称为近似方法。DQN 算法便可以用来解决连续状态下离散动作的问题。

#### CartPole 环境
![[Pasted image 20250630152808.png|500]]
#### DQN
借用深度神经网络来进行函数拟合，将用于拟合函数 $Q$ 函数的神经网络称为 $Q$ 网络

那么 Q 网络的损失函数是什么呢？我们先来回顾一下 Q-learning 的更新规则 (参见 5.5 节)：

$$Q(s, a) \leftarrow Q(s, a) + \alpha \left[ r + \gamma \max_{a' \in A} Q(s', a') - Q(s, a) \right]$$

上述公式用时序差分 (temporal difference, TD) 学习目标 $r + \gamma \max_{a' \in A} Q(s', a')$ 来增量式更新 $Q(s, a)$，也就是说要使 $Q(s, a)$ 和 TD 目标 $r + \gamma \max_{a' \in A} Q(s', a')$ 靠近。于是，对于一组数据 $\{(s_i, a_i, r_i, s'_i)\}$，我们可以很自然地将 Q 网络的损失函数构造为均方误差的形式：

$$\omega^* = \arg\min_{\omega} \frac{1}{2N} \sum_{i=1}^{N} \left[ Q_{\omega}(s_i, a_i) - \left(r_i + \gamma \max_{a'} Q_{\omega}(s'_i, a')\right) \right]^2$$

至此，我们就可以将 Q-learning 扩展到神经网络形式——深度 Q 网络 (deep Q network, DQN) 算法。由于 DQN 是离线策略算法，因此我们在收集数据的时候可以使用一个 $\epsilon$-贪婪策略来平衡探索与利用，将收集到的数据存储起来，在后续的训练中使用。DQN 中还有两个非常重要的模块——经验回放和目标网络，它们能够帮助 DQN 取得稳定、出色的性能。
##### 经验回放
在一般的有监督学习中，假设训练数据是独立同分布的，我们每次训练神经网络的时候从训练数据中随机采样一个或若干个数据来进行梯度下降，随着学习的不断进行，每一个训练数据会被使用多次。在原来的 Q-learning 算法中，每一个数据只会用来更新一次 $Q$ 值。为了更好地将 Q-learning 和深度神经网络结合，DQN 算法采用了**经验回放**（experience replay）方法，具体做法为维护一个**回放缓冲区**，将每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到回放缓冲区中，训练 Q 网络的时候再从回放缓冲区中随机采样若干数据来进行训练。这么做可以起到以下两个作用。

（1）使样本满足独立假设。在 MDP 中交互采样得到的数据本身不满足独立假设，因为这一时刻的状态和上一时刻的状态有关。非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以打破样本之间的相关性，让其满足独立假设。

（2）提高样本效率。每一个样本可以被使用多次，十分适合深度神经网络的梯度学习。

##### 目标网络
在深度Q网络（DQN）中，只使用一个网络进行训练会导致过程很不稳定，而引入第二个“目标网络”则能有效解决这个问题，让训练更稳定、更可靠。

**详细说明如下：**

1. **单一网络的问题（不稳定的根源）：**
    
    - 在Q学习中，我们需要让网络预测的Q值去逼近一个“目标Q值”。
        
    - 如果只用一个网络，那么这个“目标Q值”本身也是由这个正在被实时更新的网络计算出来的。
        
    - 这就产生了一个矛盾：网络每更新一步，它所追逐的那个“目标”也跟着变动一下。这好比**“追逐一个移动的靶子”**，使得训练过程非常容易产生震荡，难以收敛。
        
2. **双网络架构的解决方案（如何稳定下来）：**
    
    - **引入两个网络**：
        
        - **主网络 (Main Network)**：这个网络在训练的每一步都进行参数更新，主要负责学习和决策。
            
        - **目标网络 (Target Network)**：这个网络的结构和主网络完全一样，但它的参数在一段时间内是**“冻结”不变的**。它专门用来计算前面提到的“目标Q值”。
            
    - **稳定训练过程**：
        
        - 因为目标网络是冻结的，所以它能提供一个**稳定的、固定的目标**，让主网络可以安心地朝这个方向去学习和优化。这就像把移动的靶子先固定住，让射手可以稳定地练习。
            
        - 这种方法打破了“目标”和“学习者”之间的直接依赖关系，大大降低了训练的不稳定性。
            
    - **定期同步**：目标网络不能永远不变，否则它就无法代表最新的学习成果了。因此，每隔固定的步数（例如图片中提到的C步），就会将主网络最新的参数**复制**给目标网络，相当于把靶子移动到一个更能代表当前水平的新位置，然后继续稳定练习。
        

**总结一句话：** 通过增加一个参数被阶段性固定的“目标网络”，DQN成功地将一个不稳定的“动态追逐问题”转化为了一个更稳定的“阶段性静态学习问题”，从而显著提升了算法的稳定性和性能。