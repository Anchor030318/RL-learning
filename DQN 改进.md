#### Double DQN
普通的 DQN 算法通常会导致对Q值的过高估计（overestimation）。传统 DQN 优化的 TD 误差目标为

$$r + \gamma \max_{a'} Q_\omega^-(s', a')$$

其中$\max_{a'} Q_\omega^-(s', a')$由目标网络（参数为$\omega^-$）计算得出。我们还可以将其写成如下形式：

$$Q_\omega^-(s', \arg \max_{a'} Q_\omega^-(s', a'))$$

换句话说，$max$操作实际可以被拆解为两部分：**首先选取状态$s'$下的最优动作**$a^* = \arg \max_{a'} Q_\omega^-(s', a')$，**接着计算该动作对应的价值$Q_\omega^-(s', a^*)$**。当这两部分采用同一套 Q 网络进行计算时，每次得到的都是神经网络当前估算的所有动作价值中的最大值。考虑到通过神经网络估算的Q值本身存在些许的候选产生正向页向的误差，在 DQN 的更新方式下神经网络会将正向误差累积，例如，我们考虑一个特殊情形：在状态$s'$下所有动作的Q值均为 0，即$Q(s', a_i) = 0, \forall i$，此时正确的更新目标应为$r + 0 = r$，但是由于神经网络拟合的误差通常会出现某些动作的估算有正误差的情况，即存在某个动作$i$有$Q(s', a_i') > 0$，此时我们的更新目标就成了过高估计，$r + \gamma \max Q > r + 0$。因此，当我们用 DQN 的更新公式进行更新时，$Q(s,a)$也就会被过高估了了。同理，我们拿这个$Q(s,a)$来作为更新目标来更新上一步的Q值时，同样会过高估计，这样的误差将会逐步累积。对于动作空间较大的任务，DQN 的过高估计问题会非常严重，造成 DQN 无法有效工作的后果。

为了解决这一问题，Double DQN 算法提出利用两个独立训练的神经网络估算$\max_{a'} Q_\phi(s', a')$。具体做法是将原有的$\max_{a'} Q_\omega^-(s', a')$更改为$Q_\omega^-(s', \arg \max_{a'} Q_\omega(s', a'))$，即利用一套神经网络$Q_\omega$的输出选取价值最大的动作，但在使用该动作的价值时，用另一套神经网络$Q_{\omega^-}$计算该动作的价值。这样，即使其中一套神经网络的某个动作存在比较严重的过高估计问题，由于另一套神经网络的存在，这个动作最终使用的Q值不会存在很大的过高估计问题。

在传统的 DQN 算法中，本来就存在两套Q网络的神经网络——目标网络和训练网络（参见 7.3.2 节），只不过$\max_{a'} Q_\omega^-(s', a')$的计算只用到了其中的目标网络，那么我们倒好可以直接将训练网络作为 Double DQN 算法中的第一套神经网络来选动作，将目标网络作为第二套神经网络计算Q值，这便是 Double DQN 的主要思想。由于在 DQN 算法中将训练网络的参数记为$\omega$，将目标网络的参数记为$\omega^-$，这与本节中 Double DQN 的两套神经网络的参数是统一的，因此，我们可以直接写出如下 Double DQN 的优化目标：

$$r + \gamma Q_{\omega^-}(s', \arg \max_{a'} Q_\omega(s', a'))$$
#### Dueling DQN
Dueling DQN 是 DQN 另一种的改进算法，它在传统 DQN 的基础上只进行了微小的改动，但能够大幅提升 DQN 的表现。在强化学习中，我们将状态动作价值函数Q可以状态价值函数V和优势函数A，即

$$A(s,a) = Q(s,a) - V(s)$$

在同一个状态下，所有动作的优势值之和为 0，因为所有动作的动作价值的期望就是这个状态的状态价值。据此，在 Dueling DQN 中，Q 网络被建模为：

$$Q_{n,a,\beta}(s,a) = V_{n,a}(s) + A_{n,\beta}(s,a)$$

其中，$V_{n,a}(s)$为状态价值函数，而$A_{n,\beta}(s,a)$则为该状态下采取不同动作的优势函数，表示采取不同动作的差异性；$n$表状态价值函数和优势函数共享的网络参数，一般用在神经网络中，用来提取特征的前几层；而$\alpha$和$\beta$分别为状态价值函数和优势函数的参数。在这样的模型下，我们不再让神经网络直接输出Q值，而是训练神经网络的最后几层的两个分支，分别输出状态价值函数和优势函数，再求得到Q值。Dueling DQN 的网络结构是 VAnet，前半部分共享参数，后半部分分别学习状态价值函数和优势函数

将状态价值函数和优势函数分别建模的好处在于：某些情境下智能体只会关心与状态相关的价值，而并不关心不同动作导致的差异，此时将二者分开建模能够使智能体更好地处理与动作关联较小的状态。

对于 Dueling DQN 中的公式$Q_{n,a,\beta}(s,a) = V_{n,a}(s) + A_{n,\beta}(s,a)$，它存在对于V值和A值建模不唯一性的问题。例如，**对于同样的Q值，如果V值加上任意常数$C$，再将所有A值减去$C$，则得到的Q值依然不变。这就意味着训练的不稳定性**。为了解决这一问题，**Dueling DQN 强制最优动作的优势函数的关键输出为 0**，即：

$$Q_{n,a,\beta}(s,a) = V_{n,a}(s) + A_{n,\beta}(s,a) - \max_{a'} A_{n,\beta}(s,a')$$

此时$V(s) = \max_a Q(s,a)$，可以确保V值建模的唯一性。在实现过程中，我们还可以用**平均代替最大化操作**，即：

$$Q_{n,a,\beta}(s,a) = V_{n,a}(s) + A_{n,\beta}(s,a) - \frac{1}{|A|} \sum_{a'} A_{n,\beta}(s,a')$$

此时$V(s) = \frac{1}{|A|} \sum_{a'} Q(s,a')$。在下面的代码实现中，我们将采取此种方式，虽然它不满足定义所需要的最优性，但实际应用时更加稳定。

有的读者可能会问："为什么 Dueling DQN 会比 DQN 好？"部分原因在于 Dueling DQN 能更高效学习状态价值函数。每一次更新时，函数V都会被更新，这也会影响到其他动作的Q值。而传统的 DQN 只会更新某一动作的Q值，其他动作的Q值就不会被更新。因此，**Dueling DQN 能够更频繁、准确地更新状态价值函数。**

