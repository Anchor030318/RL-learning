基于动态规划的强化学习算法主要有两种：一是**策略迭代**（policy iteration），二是**价值迭代**（value iteration）。其中，策略迭代由两部分组成：**策略评估**（policy evaluation）和**策略提升**（policy improvement）。

*基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。*

*但是，现实中的白盒环境很少，这也是动态规划算法的局限之处，我们无法将其运用到很多实际场景中。*

*另外，策略迭代和价值迭代通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的。*

#### 策略迭代算法
##### 策略评估
策略评估这一过程用来计算一个策略的状态价值函数。回顾一下之前学习的贝尔曼期望方程： $$V^{\pi}(s) = \sum_{a \in A} \pi(a|s) \left( r(s,a) + \gamma \sum_{s' \in S} P(s'|s,a) V^{\pi}(s') \right) $$ 其中, $\pi(a|s)$ 是策略 $\pi$ 在状态 $s$ 下采取动作 $a$ 的概率。可以看到，当知道奖励函数和状态转移函数时，我们可以根据下一个状态的价值来计算当前状态的价值。因此，根据动态规划的思想，可以把计算下一个可能状态的价值当成一个子问题，把计算当前状态的价值看作当前问题。在得知子问题的解后，就可以求解当前问题。更一般的，考虑所有的状态，就变成了用上一轮的状态价值函数来计算当前这一轮的状态价值函数，即 $$V^{k+1}(s) = \sum_{a \in A} \pi(a|s) \left( r(s,a) + \gamma \sum_{s' \in S} P(s'|s,a) V^k(s') \right) $$ 我们可以选定任意初始值 $V^0$。根据贝尔曼期望方程，可以得知 $V^k = V^{\pi}$ 是以上更新公式的一个不动点 (fixed point)。事实上，可以证明当 $k \to \infty$ 时，序列 $\{V^k\}$ 会收敛到 $V^{\pi}$，所以可以据此来计算得到一个策略的状态价值函数。可以看到，由于需要不断做贝尔曼期望方程迭代，策略评估其实会耗费很大的计算代价。在实际的实现过程中，如果某一轮 $\max_{s \in S} |V^{k+1}(s) - V^k(s)|$ 的值非常小，可以提前结束策略评估。这样做可以提升效率，并且得到的值也非常接近真实的值。
##### 策略提升
使用策略评估计算得到当前策略的状态价值函数之后，我们便可以据此来改进该策略。假设此时对于策略 $\pi$，我们已经知道其价值 $V^{\pi}$，也就是知道了在策略 $\pi$ 下从每一个状态 $s$ 出发最终得到的期望回报。我们要如何改变策略来获得在状态 $s$ 下更高的期望回报呢？假设智能体在状态 $s$ 下采取动作 $a$，之后的动作依旧遵循策略 $\pi$，此时得到的期望回报其实就是动作价值 $Q^{\pi}(s, a)$。如果我们有 $Q^{\pi}(s, a) > V^{\pi}(s)$，则说明在状态 $s$ 下采取动作 $a$ 会比原来的策略 $\pi(a|s)$ 得到更高的期望回报。以上假设只是针对一个状态，现在假设存在一个确定性策略 $\pi'$，在任意一个状态 $s$ 下，都满足 $$ Q^{\pi}(s, \pi'(s)) \ge V^{\pi}(s) $$ 于是，在任意状态 $s$ 下，我们有 $$ V^{\pi'}(s) \ge V^{\pi}(s) $$ 这便是策略提升定理 (policy improvement theorem)。于是我们可以直接贪心地在每一个状态选择动作价值最大的动作，也就是 $$\pi'(s) = \underset{a}{\arg\max} \, Q^{\pi}(s, a) = \underset{a}{\arg\max} \left\{ r(s, a) + \gamma \sum_{s'} P(s'|s, a) V^{\pi}(s') \right\} $$ 我们发现构造的贪心策略 $\pi'$ 满足策略提升定理的条件，所以策略 $\pi'$ 能够比策略 $\pi$ 更好或者至少与其一样好。这个根据贪心法选取动作从而得到新策略的过程称为策略提升。当策略提升之后得到的新策略 $\pi'$ 和之前的策略 $\pi$ 一样时，说明策略迭代达到了收敛，此时 $\pi$ 和 $\pi'$ 就是最优策略。 策略提升定理的证明通过以下推导过程可以证明，使用上述公式得到的新策略 $\pi'$ 在每个状态的价值不低于原策略 $\pi$ 在该状态的价值。 $$\begin{align*} V^{\pi}(s) &\le Q^{\pi}(s, \pi'(s)) \\ &= \mathbb{E}_{\pi'}[R_{t+1} + \gamma V^{\pi}(S_{t+1}) | S_t = s] \\ &\le \mathbb{E}_{\pi'}[R_{t+1} + \gamma Q^{\pi}(S_{t+1}, \pi'(S_{t+1})) | S_t = s] \\ &= \mathbb{E}_{\pi'}[R_{t+1} + \gamma \mathbb{E}_{\pi'}[R_{t+2} + \gamma V^{\pi}(S_{t+2}) | S_{t+1}] | S_t = s] \\ &= \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 V^{\pi}(S_{t+2}) | S_t = s] \\ &\le \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 V^{\pi}(S_{t+3}) | S_t = s] \\ &\vdots \\ &\le \mathbb{E}_{\pi'}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots | S_t = s] \\ &= V^{\pi'}(s) \end{align*}$$ 可以看到，推导过程中的每一个时间步都用到局部动作价值优势 $V^{\pi}(S_{t+1}) \le Q^{\pi}(S_{t+1}, \pi'(S_{t+1}))$，累积到无穷步或者终止状态时，我们就得到了整个策略价值函数的不等式。

总体来说，策略迭代算法的过程如下：对当前的策略进行策略评估，得到其状态价值函数，然后根据该状态价值函数进行策略提升以得到一个更好的新策略，接着继续评估新策略、提升策略...直至最后收敛到最优策略（收敛性证明参见 4.7 节）： $$\pi^0 \xrightarrow{\text{策略评估}} V^{\pi^0} \xrightarrow{\text{策略提升}} \pi^1 \xrightarrow{\text{策略评估}} V^{\pi^1} \xrightarrow{\text{策略提升}} \pi^2 \xrightarrow{\text{策略评估}} \dots \xrightarrow{\text{策略提升}} \pi^* $$