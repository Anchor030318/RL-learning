之前的 Q-learning、DQN 以及 DQN 改进算法都是**基于价值**的方法。Q-learning 是处理有限状态的算法，DQN 可以解决连续状态的问题。

除了基于值函数的方法，还有基于策略的方法。

对比两者，基于值函数的方法主要是**学习值函数，然后根据值函数导出一个策略**，学习过程中并不存在一个显式的策略；而基**于策略的方法则是直接显式地学习一个目标策略**。

#### 策略梯度算法
首先需要将策略参数化。假设目标策略 $\pi_{\theta}$ 是一个随机性策略，并且处处可微，其中 $\theta$ 是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。我们的目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。我们将策略学习的目标函数定义为 ：（也就是当前策略在初始状态的期望回报）$$J(\theta) = \mathbb{E}_{s_0}[V^{\pi_\theta}(s_0)]$$ 其中，$s_0$ 表示初始状态。现在有了目标函数，我们将目标函数对策略 $\theta$ 求导，得到导数后，就可以用梯度上升方法来最大化这个目标函数，从而得到最优策略。 我第 3 章讲解过策略 $\pi$ 下的状态访问分布，在此用 $\nu^\pi$ 表示。然后我们对目标函数求梯度，可以得到如下式子，更详细的推导过程将在 9.6 节给出。 $$ \begin{aligned} \nabla_\theta J(\theta) & \propto \sum_{s \in S} \nu^{\pi_\theta}(s) \sum_{a \in A} Q^{\pi_\theta}(s, a) \nabla_\theta \pi_\theta(a|s) \\ & = \sum_{s \in S} \nu^{\pi_\theta}(s) \sum_{a \in A} \pi_\theta(a|s) Q^{\pi_\theta}(s, a) \frac{\nabla_\theta \pi_\theta(a|s)}{\pi_\theta(a|s)} \\ & = \mathbb{E}_{\pi_\theta}[Q^{\pi_\theta}(s, a) \nabla_\theta \log \pi_\theta(a|s)] \end{aligned} $$ 这个梯度可以用来更新策略。需要注意的是，因为上式中期望的下标是 $\pi_\theta$，所以策略梯度算法为在线策略 (on-policy) 算法，即必须使用当前策略 $\pi_\theta$ 采样得到的数据来计算梯度。

#### REINFORCE 算法
在计算策略梯度的公式中，我们需要用到 $Q^{\pi_\theta}(s,a)$，可以用多种方式对它进行估计。接下来要介绍的 REINFORCE 算法便是采用了蒙特卡洛方法来估计 $Q^{\pi_\theta}(s,a)$，对于一个有限步数的环境来说，REINFORCE 算法中的策略梯度为： $$\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \left( \sum_{t'=t}^T \gamma^{t'-t} r_{t'} \right) \nabla_\theta \log \pi_\theta (a_t | s_t) \right]$$ 其中，$T$ 是和环境交互的最大步数。例如，在车杆环境中，$T = 200$。 
##### 9.3 REINFORCE 
REINFORCE 算法的具体算法流程如下： 
* 初始化策略参数 $\theta$ 
* for 序列 $e=1 \rightarrow E$ do: 
	* 用当前策略 $\pi_\theta$ 采样轨迹 $\{s_1, a_1, r_1, s_2, a_2, r_2, \dots, s_T, a_T, r_T\}$ 
	* 计算当前轨迹每个时刻 $t$ 往后的回报 $\sum_{t'=t}^T \gamma^{t'-t} r_{t'}$，记为 $\psi_t$ 
	* 对 $\theta$ 进行更新, $\theta = \theta + \alpha \sum_t \psi_t \nabla_\theta \log \pi_\theta(a_t|s_t)$ 
* end for

#### 策略梯度定理证明

##### 定理陈述

策略梯度定理：对于**任何可微分策略 $π(a|s,θ)$**，策略梯度为：

$$∇_θ J(θ) = E_{s∼d^π,a∼π}[∇_θ \log π(a|s,θ) Q^π(s,a)]$$

其中 $d^π(s)$ 是在策略 π 下的状态分布。

## 证明过程

### 第一步：从状态价值函数开始

状态价值函数定义： $$V^π(s) = E_{a∼π}[Q^π(s,a)] = \sum_a π(a|s,θ) Q^π(s,a)$$

### 第二步：计算状态价值函数的梯度

$$∇_θ V^π(s) = ∇_θ \sum_a π(a|s,θ) Q^π(s,a)$$

$$= \sum_a [∇_θ π(a|s,θ) Q^π(s,a) + π(a|s,θ) ∇_θ Q^π(s,a)]$$

### 第三步：利用对数梯度技巧

注意到 $∇_θ π(a|s,θ) = π(a|s,θ) ∇_θ \log π(a|s,θ)$，因此：

$$∇_θ V^π(s) = \sum_a [π(a|s,θ) ∇_θ \log π(a|s,θ) Q^π(s,a) + π(a|s,θ) ∇_θ Q^π(s,a)]$$

### 第四步：处理 Q 函数的梯度

$Q^π(s,a)$ 的定义： $$Q^π(s,a) = E[R_{t+1} + γ V^π(S_{t+1}) | S_t=s, A_t=a]$$

因此： $$∇_θ Q^π(s,a) = E[γ ∇_θ V^π(S_{t+1}) | S_t=s, A_t=a]$$

$$= γ \sum_{s'} P(s'|s,a) ∇_θ V^π(s')$$

### 第五步：递归展开

将第四步结果代入第三步：

$$∇_θ V^π(s) = \sum_a π(a|s,θ) [∇_θ \log π(a|s,θ) Q^π(s,a) + γ \sum_{s'} P(s'|s,a) ∇_θ V^π(s')]$$

### 第六步：定义辅助函数

为简化表示，定义： $$Φ(s) = \sum_a π(a|s,θ) ∇_θ \log π(a|s,θ) Q^π(s,a)$$

则有： $$∇_θ V^π(s) = Φ(s) + γ \sum_a π(a|s,θ) \sum_{s'} P(s'|s,a) ∇_θ V^π(s')$$

$$= Φ(s) + γ \sum_{s'} P^π(s→s') ∇_θ V^π(s')$$

其中 $P^π(s→s') = \sum_a π(a|s,θ) P(s'|s,a)$ 是在策略 π 下从状态 s 转移到状态 s' 的概率。

### 第七步：递归展开求解

递归展开上式：

$$∇_θ V^π(s) = Φ(s) + γ \sum_{s'} P^π(s→s') [Φ(s') + γ \sum_{s''} P^π(s'→s'') ∇_θ V^π(s'')]$$

$$= Φ(s) + γ \sum_{s'} P^π(s→s') Φ(s') + γ^2 \sum_{s''} P^π(s→s'') ∇_θ V^π(s'')$$

继续展开：

$$= \sum_{k=0}^∞ γ^k \sum_{s'} P^π(s→s', k) Φ(s')$$

其中 $P^π(s→s', k)$ 表示在策略 π 下从状态 s 经过 k 步转移到状态 s' 的概率。

### 第八步：引入状态访问分布

定义状态访问分布： $$d^π(s') = \sum_{k=0}^∞ γ^k P^π(s_0→s', k)$$

其中 $s_0$ 是初始状态。

### 第九步：得到最终结果

对于目标函数 $J(θ) = V^π(s_0)$：

$$∇_θ J(θ) = ∇_θ V^π(s_0) = \sum_{s'} d^π(s') Φ(s')$$

$$= \sum_{s'} d^π(s') \sum_a π(a|s',θ) ∇_θ \log π(a|s',θ) Q^π(s',a)$$

$$= E_{s∼d^π,a∼π}[∇_θ \log π(a|s,θ) Q^π(s,a)]$$

## 证明完毕

这就是策略梯度定理的完整证明。该定理表明，策略梯度等于在策略分布下对数策略梯度与动作价值函数乘积的期望。